{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeea91a2",
   "metadata": {},
   "source": [
    "# Measuring US Industry Level Productivity for 1947-2023\n",
    "## [Juan Ignacio Vizcaino](https://www.jivizcaino.com/) and [Selim Elbadri](https://www.selimelbadri.com/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc79bbe",
   "metadata": {},
   "source": [
    "We here combine data from US KLEMS, March 2017 Release, with BEA-BLS Integrated Industry-Level Production Account for 1947–2016, and BEA-BLS Integrated Industry-Level Production Account for 1997-2023 to produce industry-level measures of Gross Output (GO), Value Added (VA), Capital (CAP), Labor (LAB), and Intermediate Inputs (II), in Nominal and Real terms. We also provide Quantity Indices for GO, VA, CAP, LAB, and II, and utilize these indices to compute measures of Total Labor Productivity (LP) and Total Factor Productivity (TFP) for the corresponding time period, following the methodology in US KLEMS, April 2013 Release.\n",
    "See our [Gihub repo](https://github.com/selbadri/Measuring-US-Industry-Level-Productivity-1947-to-2023) for details on data processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d5ffb",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf023ce",
   "metadata": {},
   "source": [
    "#### 1.1 Set up the Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "790b0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_file_path = rf\"..\\\\Input\"\n",
    "export_file_path = rf\"..\\\\Output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c8a2a",
   "metadata": {},
   "source": [
    "#### 1.2. Requirements: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1312efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b09a3",
   "metadata": {},
   "source": [
    "#### 1.3. Define Frequently Used Functions and Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd9bf3",
   "metadata": {},
   "source": [
    "Define some widely used functions (see docstring for details on what these functions do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "518ea8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tornqvist_index(df, q_vars, v_vars, industry_column=None):\n",
    "    \"\"\"\n",
    "    This function calculatest a Tornqvist quantity index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing quantity and value variables\n",
    "    q_vars : list\n",
    "        List of column names representing quantity variables to be aggregated\n",
    "    v_vars : list  \n",
    "        List of column names representing corresponding value variables\n",
    "        used for weighting. Must be same length as q_vars.\n",
    "    industry_column : str, optional\n",
    "        Column name for industry/group identifier. If provided, calculations\n",
    "        are performed separately for each industry group. If None, treats\n",
    "        entire dataset as single time series.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Tornqvist quantity index values, normalized to 100 for the first\n",
    "        observation of each group (or first row if no grouping)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The Tornqvist index formula used is:\n",
    "    ln(QI_t/QI_{t-1}) = Σ [0.5 * (w_i,t + w_i,t-1) * ln(q_i,t/q_i,t-1)]\n",
    "    \n",
    "    Where:\n",
    "    - QI_t is the quantity index at time t\n",
    "    - w_i,t is the value share of component i at time t (v_i,t / Σv_j,t)\n",
    "    - q_i,t is the quantity of component i at time t\n",
    "    \n",
    "    The index is initialized to 100 for the first period and cumulative\n",
    "    products are calculated to generate the full time series.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Calculate aggregate capital quantity index\n",
    "    >>> capital_qi = tornqvist_index(\n",
    "    ...     df, \n",
    "    ...     q_vars=['it_quantity', 'structures_quantity', 'equipment_quantity'],\n",
    "    ...     v_vars=['it_value', 'structures_value', 'equipment_value'],\n",
    "    ...     industry_column='industry_id'\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    total_v = df[v_vars].sum(axis=1)\n",
    "    for col in v_vars:\n",
    "        df[f'w_{col}'] = df[col] / total_v\n",
    "        \n",
    "    log_index_sum = 0\n",
    "    for q_var, v_var in zip(q_vars, v_vars):\n",
    "        w_var = f'w_{v_var}'\n",
    "        \n",
    "        if industry_column is not None:\n",
    "            log_change = np.log(df[q_var] / df.groupby(industry_column)[q_var].shift(1))\n",
    "            avg_weight = 0.5 * (df[w_var] + df.groupby(industry_column)[w_var].shift(1))\n",
    "        else:\n",
    "            log_change = np.log(df[q_var] / df[q_var].shift(1))\n",
    "            avg_weight = 0.5 * (df[w_var] + df[w_var].shift(1))\n",
    "\n",
    "        log_index_sum += avg_weight * log_change\n",
    "\n",
    "    q_growth_rate = np.exp(log_index_sum)\n",
    "\n",
    "    if industry_column is not None:\n",
    "        q_growth_rate.loc[df.groupby(industry_column).head(1).index] = 100\n",
    "        qi = q_growth_rate.groupby(df[industry_column]).cumprod()\n",
    "    else:\n",
    "        q_growth_rate.iloc[0] = 100\n",
    "        qi = q_growth_rate.cumprod()\n",
    "\n",
    "    return qi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5704161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(df, variable, year, industry_column):\n",
    "    \"\"\"Normalize variable to base year = 100 by industry.\"\"\"\n",
    "    base_values = df[df['year'] == year].set_index(industry_column)[variable]\n",
    "    normaliser = df[industry_column].map(base_values)\n",
    "    df[variable] = (df[variable] / normaliser) * 100\n",
    "    return df[variable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d67123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(obj):\n",
    "    \"\"\"\n",
    "    Standardize names by replacing spaces with underscores and converting to lowercase.\n",
    "    This function is used for making column names consistent across different data sources.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obj : pd.DataFrame, list, or str\n",
    "        Object to clean\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Same type as input with standardized naming\n",
    "    \"\"\"\n",
    "    if isinstance(obj, pd.DataFrame):\n",
    "        obj.columns = [col.replace(' ', '_').lower() for col in obj.columns]\n",
    "        return obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [s.replace(' ', '_').lower() for s in obj]\n",
    "    elif isinstance(obj, str):\n",
    "        return obj.replace(' ', '_').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b98a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_industries(df):\n",
    "    \"\"\"\n",
    "    Create industry aggregates by combining individual industries into larger sectoral groups.\n",
    "    \n",
    "    This function is used to produce consistent industry aggregates from 1947-2023, given that the\n",
    "    BEA-BLS Industry Production Account Experimental file has aggregated data for industries 29-36,\n",
    "    37-40, 41-44, 47-49, 51-52, 54-56, and 57-58.\n",
    "    \n",
    "    The function creates aggregate industry groups (e.g., combining industries 29-36 into group 2936) for 1947-2023 \n",
    "    and computes appropriate Tornqvist quantity indices that maintain economic consistency.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe with MultiIndex ['year', 'industry_id'] containing individual\n",
    "        industry data with quantity indices and value added measures.\n",
    "        Required columns: qi_variables (quantity indices) and 'VA' (value added)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Original dataframe with additional rows for aggregate industries, each containing\n",
    "        appropriately calculated Tornqvist quantity indices for the combined sectors.\n",
    "        MultiIndex structure ['year', 'industry_id'] is preserved.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The function operates by:\n",
    "    1. Creating industry groups according to aggregate_groups mapping (see list below)\n",
    "    2. For each group and each quantity index variable:\n",
    "       - Extracting component industry data (quantities and values)\n",
    "       - Applying Tornqvist aggregation formula using individual industry weights\n",
    "       - Assigning new aggregate industry code to results\n",
    "    3. Concatenating aggregate results with original individual industry data\n",
    "    \n",
    "    The Tornqvist aggregation ensures that the aggregate indices properly reflect\n",
    "    the relative economic importance of component industries through value-based weighting.\n",
    "    \n",
    "    Dependencies\n",
    "    ------------\n",
    "    - Requires global variables: qi_variables, aggregate_groups\n",
    "    - Uses tornqvist_index() function for proper economic aggregation\n",
    "    - Assumes df contains 'VA' (Value Added) column for weighting\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Typical usage in productivity analysis pipeline:\n",
    "    >>> # After calculating individual industry quantity indices\n",
    "    >>> df_with_aggregates = aggregate_industries(df_individual)\n",
    "    >>> # Result contains both individual industries (1, 2, 3, ...) \n",
    "    >>> # and aggregates (2936, 3740, 4144, ...)\n",
    "    \"\"\"\n",
    "    aggregate_dict = {}\n",
    "    df = df.reset_index()\n",
    "    for qi in qi_variables:\n",
    "        for agg_code, industries in aggregate_groups.items():\n",
    "            data_slice = df[df['industry_id'].isin(industries)].copy()\n",
    "\n",
    "            q_vars = []\n",
    "            v_vars = []\n",
    "\n",
    "            for industry in industries:\n",
    "                q = f'{industry}_{qi}'\n",
    "                v = f'{industry}_va'\n",
    "\n",
    "                services_index = df[df['industry_id'] == industry][['year', qi]].rename(columns={qi: q})\n",
    "                va = df[df['industry_id'] == industry][['year', 'VA']].rename(columns={'VA': v})\n",
    "            \n",
    "                data_slice = data_slice.merge(services_index, on='year', how='left')\n",
    "                data_slice = data_slice.merge(va, on='year', how='left')\n",
    "            \n",
    "                q_vars.append(q)\n",
    "                v_vars.append(v)\n",
    "\n",
    "            sum_cols = ['VA']\n",
    "            data_slice[sum_cols] = data_slice.groupby('year')[sum_cols].transform('sum')\n",
    "            data_slice = data_slice.drop_duplicates(subset='year')\n",
    "\n",
    "            data_slice[qi] = tornqvist_index(data_slice, q_vars=q_vars, v_vars=v_vars, industry_column='industry_id')\n",
    "            data_slice['industry_id'] = agg_code\n",
    "\n",
    "            if agg_code not in aggregate_dict:\n",
    "                aggregate_dict[agg_code] = data_slice[['year', 'industry_id'] + [qi] + sum_cols]\n",
    "            else:\n",
    "                aggregate_dict[agg_code] = aggregate_dict[agg_code].merge(data_slice[['year', 'industry_id', qi]], on=['year', 'industry_id'], how='left')\n",
    "\n",
    "    aggregate_df = pd.concat(aggregate_dict.values(), ignore_index=True)\n",
    "\n",
    "    df = pd.concat([df, aggregate_df], ignore_index=True).set_index(['year', 'industry_id'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e3150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain(df_1, df_2, year):\n",
    "    \"\"\"\n",
    "    Chain together two time series datasets with overlapping periods using index linking.\n",
    "    \n",
    "    This function is used for creating consistent long-term economic time series \n",
    "    when combining data from different sources or periods. It ensures continuity by\n",
    "    scaling the second dataset to match the level of the first dataset at the \n",
    "    overlapping year, maintaining growth rates while eliminating level discontinuities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_1 : pandas.DataFrame\n",
    "        First (earlier) dataset with MultiIndex ['year', 'industry_id'].\n",
    "        Contains quantity index variables that serve as the reference level.\n",
    "    df_2 : pandas.DataFrame\n",
    "        Second (later) dataset with MultiIndex ['year', 'industry_id'].\n",
    "        Will be scaled to match df_1 levels at the linking year.\n",
    "    year : int\n",
    "        Overlapping year used for linking the two datasets. This year's values\n",
    "        from df_1 provide the scaling factors for df_2.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Combined dataset with MultiIndex ['year', 'industry_id'] containing\n",
    "        the chained time series. df_1 data (excluding linking year) plus\n",
    "        df_2 data scaled to maintain continuity.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The chaining process:\n",
    "    1. Extracts scaling factors from df_1 at the linking year\n",
    "    2. Applies these factors to all df_2 observations to maintain relative levels\n",
    "    3. Removes the linking year from df_1 to avoid duplication\n",
    "    4. Combines the datasets preserving chronological order\n",
    "    \n",
    "    The scaling formula for each quantity index (qi) is:\n",
    "    df_2_scaled[qi] = df_2[qi] * (df_1[qi, linking_year] / 100)\n",
    "    \n",
    "    This preserves growth rates in df_2 while ensuring level consistency with df_1.\n",
    "    \n",
    "    Dependencies\n",
    "    ------------\n",
    "    - Requires global variable: qi_variables (list of quantity index column names)\n",
    "    - Both datasets must have matching industry_id values for proper scaling\n",
    "    - Missing scaling factors are filled with 100 (no adjustment)\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Typical usage in productivity analysis:\n",
    "    >>> # Chain 1947-1996 data with 1997-2023 data using 1997 as link year\n",
    "    >>> combined_data = chain(early_period_df, later_period_df, 1997)\n",
    "    >>> # Result: consistent time series from 1947-2023\n",
    "    \"\"\"\n",
    "    df_1 = df_1.reset_index()\n",
    "    df_2 = df_2.reset_index()\n",
    "\n",
    "    df_2_scaled = df_2.copy()\n",
    "\n",
    "    for qi in qi_variables:\n",
    "        scalers = f'{qi}_scalers'\n",
    "\n",
    "        df_1_year = df_1[df_1['year'] == year][['industry_id', qi]].rename(columns={qi: scalers})\n",
    "\n",
    "        df_2_scaled = df_2_scaled.merge(df_1_year, on='industry_id', how='left')\n",
    "        df_2_scaled[scalers] = df_2_scaled[scalers].fillna(100)\n",
    "        df_2_scaled[qi] *= df_2_scaled[scalers]\n",
    "        df_2_scaled[qi] = df_2_scaled[qi] / 100\n",
    "        df_2_scaled = df_2_scaled.drop(columns=scalers)\n",
    "\n",
    "    df_1 = df_1[df_1['year'] != year]\n",
    "\n",
    "    df_new = pd.concat([df_1, df_2_scaled], ignore_index=True)\n",
    "    df_new = df_new.set_index(['year', 'industry_id']).sort_values(by=['industry_id', 'year'])\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a83f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_values(df, variable, year):\n",
    "    \"\"\"\n",
    "    Convert nominal values to real (constant dollar) values using implicit price deflator.\n",
    "    \n",
    "    This function implements the standard economic method for converting nominal time series \n",
    "    into real values by extracting implicit price indices and deflating current values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing nominal values and corresponding quantity indices.\n",
    "        Must include columns: 'year', 'industry_id', variable, and f'{variable}_QI'\n",
    "    variable : str\n",
    "        Name of the nominal variable to convert (e.g., 'GO', 'CAP', 'LAB', 'II').\n",
    "        Must have a corresponding quantity index column f'{variable}_QI'\n",
    "    year : int\n",
    "        Base year for the constant dollar calculation. Real values will be expressed\n",
    "        in this year's dollars (e.g., 2009 for \"2009 constant dollars\")\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Real (constant dollar) values for the specified variable, rounded to 4 decimals.\n",
    "        Series name will be f'REAL_{variable}' (e.g., 'REAL_GO')\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The conversion process follows these steps:\n",
    "    \n",
    "    1. Create value index:           V_index(t) = V(t) / V(base_year) × 100\n",
    "    2. Extract implicit price index: P_index(t) = V_index(t) / Q_index(t)  \n",
    "    3. Calculate real values:        REAL_V(t)  = V(t) / P_index(t)\n",
    "    \n",
    "    Where:\n",
    "    - V(t)       = nominal value at time t\n",
    "    - Q_index(t) = quantity index at time t  \n",
    "    - P_index(t) = implicit price index at time t\n",
    "    \n",
    "    This method ensures that real values reflect only quantity changes, with price\n",
    "    effects removed through the implicit deflation process.\n",
    "    \n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Convert nominal gross output to 2009 constant dollars:\n",
    "    >>> real_go = constant_values(df, 'GO', 2009)\n",
    "    >>> # Result: REAL_GO series in 2009 constant dollars\n",
    "    \n",
    "    Convert nominal capital to 1997 constant dollars:  \n",
    "    >>> real_cap = constant_values(df, 'CAP', 1997)\n",
    "    >>> # Result: REAL_CAP series in 1997 constant dollars\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    values_year = {}\n",
    "    df_year = df[df['year'] == year].set_index('industry_id')\n",
    "    values_year = df_year[variable].to_dict()\n",
    "    df[f'{variable}_{year}'] = df['industry_id'].map(values_year)\n",
    "    df[f'{variable}_value_index'] = df[variable] / df[f'{variable}_{year}']\n",
    "    df[f'{variable}_value_index'] *= 100\n",
    "    df[f'{variable}_price_index'] = df[f'{variable}_value_index'] / df[f'{variable}_QI']\n",
    "    df[f'REAL_{variable}'] = df[variable] / df[f'{variable}_price_index']\n",
    "    df[f'REAL_{variable}'] = df[f'REAL_{variable}'].round(4)\n",
    "    return df[f'REAL_{variable}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90935562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_index(df, index_name, variable):\n",
    "    \"\"\"\n",
    "    Reconstruct level indices from cumulative log differences (growth rates).\n",
    "    \n",
    "    This function converts log difference series (growth rates) back into index levels that \n",
    "    can be normalized and compared across time periods. It is used to produce chained quantity\n",
    "    indices for when we have data on for overlapping periosds from different sources.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing log difference variables by industry.\n",
    "        Must include 'industry_id' column and f'delta_ln_{variable}' column.\n",
    "    index_name : str\n",
    "        Name for the recovered index variable (e.g., 'TFP_VA', 'LP_GO', 'VA_QI').\n",
    "        This becomes the column name for the output index.\n",
    "    variable : str\n",
    "        Base name of the log difference variable to recover from.\n",
    "        Function expects column f'delta_ln_{variable}' to exist in df.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Recovered index series normalized to 100 for base period, with name=index_name.\n",
    "        Index values represent cumulative changes from the first observation.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Mathematical Process:\n",
    "    1. Cumulative sum: ln(Index_t) = Σ(Δln(Index_s)) for s = 1 to t\n",
    "    2. Exponentiation: Index_t     = exp(ln(Index_t))  \n",
    "    3. Normalization:  Index_t     = Index_t × 100\n",
    "    \n",
    "    Where Δln(Index_t) represents the log difference (growth rate) at time t.\n",
    "    \n",
    "    The cumulative sum is performed within each industry group, ensuring that\n",
    "    each industry's index starts from its own baseline period.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Recover Total Factor Productivity index from growth rates:\n",
    "    >>> tfp_index = recover_index(df, 'TFP_VA', 'TFP_VA')\n",
    "    >>> # Creates index from delta_ln_TFP_VA growth rates\n",
    "    \n",
    "    Recover Value Added quantity index:\n",
    "    >>> va_qi = recover_index(df, 'VA_QI', 'VA_QI')  \n",
    "    >>> # Creates index from delta_ln_VA_QI growth rates\n",
    "    \"\"\"\n",
    "    \n",
    "    df['ln_' + variable] = df.groupby('industry_id')['delta_ln_' + variable].cumsum().fillna(0)\n",
    "    df[index_name] = np.exp(df['ln_' + variable])\n",
    "    df[index_name] *= 100\n",
    "    return df[index_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c273a01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag(df, variable):\n",
    "    \"\"\"\n",
    "    Create lagged (previous period) values of a variable by industry group.\n",
    "    \n",
    "    It creates one-period lags within each industry group, ensuring that lagged values \n",
    "    don't bleed across different industries in the panel data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing the variable to lag. Must include 'industry_id' column\n",
    "        for proper grouping and should be sorted by time within each industry.\n",
    "    variable : str\n",
    "        Name of the column to create lagged values for. The lagged column will be named\n",
    "        f'{variable}_lag' (e.g., 'VA/GO' becomes 'VA/GO_lag').\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        Lagged values of the specified variable, with NaN for the first observation\n",
    "        of each industry group (no previous period available).\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    Create lagged value-added share for Tornqvist calculations:\n",
    "    >>> va_go_lag = lag(df, 'VA/GO')\n",
    "    >>> # Used in: avg_share = 0.5 * (df['VA/GO'] + va_go_lag)\n",
    "    \n",
    "    Create lagged labor share:\n",
    "    >>> lab_va_lag = lag(df, 'LAB/VA') \n",
    "    >>> # Used in TFP calculations for proper factor weighting\n",
    "    \"\"\"\n",
    "    df[f'{variable}_lag'] = df.groupby('industry_id')[variable].transform(lambda x: x.shift(1))\n",
    "    return df[f'{variable}_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f7db026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(df, variable):\n",
    "    \"\"\"\n",
    "    Calculate first differences (period-over-period changes) of a variable by industry group.\n",
    "    This function computes simple arithmetic differences between consecutive periods\n",
    "    within each industry.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing the variable to difference. Must include 'industry_id' \n",
    "        column for proper grouping and should be sorted by time within each industry.\n",
    "    variable : str\n",
    "        Name of the column to calculate differences for. The difference column will be \n",
    "        named f'delta_{variable}' (e.g., 'ln_REAL_GO' becomes 'delta_ln_REAL_GO').\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.Series\n",
    "        First differences of the specified variable, with NaN for the first observation\n",
    "        of each industry group (no previous period for comparison).\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    Mathematical Operation:\n",
    "    delta_X(t) = X(t) - X(t-1)\n",
    "    \n",
    "    When applied to logarithmic variables:\n",
    "    delta_ln_X(t) = ln(X(t)) - ln(X(t-1)) = ln(X(t)/X(t-1))\n",
    "    \n",
    "    This represents the continuous growth rate, which is approximately equal to\n",
    "    the percentage change for small changes.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Calculate log growth rate of real gross output:\n",
    "    >>> go_growth = delta(df, 'ln_REAL_GO')\n",
    "    >>> # Result: delta_ln_REAL_GO = ln(GO_t) - ln(GO_{t-1})\n",
    "    \n",
    "    Calculate change in labor share:\n",
    "    >>> share_change = delta(df, 'LAB/VA') \n",
    "    >>> # Result: delta_LAB/VA = (LAB/VA)_t - (LAB/VA)_{t-1}\n",
    "    \"\"\"\n",
    "    df[f'delta_{variable}'] = df.groupby('industry_id')[variable].transform(lambda x: x - x.shift(1))\n",
    "    return df[f'delta_{variable}']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f1ece",
   "metadata": {},
   "source": [
    "Define two widely used lists. The first list *aggregate_groups* is useful to aggregate industries at the same level as the BEA-BLS Industry Production Account Experimental for the period of 1947-1963.\n",
    "The second set of lists define variable groupings that are useful for computations later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85e6d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_groups = {\n",
    "    2936: list(range(29, 37)),\n",
    "    3740: list(range(37, 41)),\n",
    "    4144: list(range(41, 45)),\n",
    "    4749: list(range(47, 50)),\n",
    "    5152: list(range(51, 53)),\n",
    "    5456: list(range(54, 57)),\n",
    "    5758: list(range(57, 59))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "529e11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_variables               = ['GO', 'CAP', 'LAB', 'II']\n",
    "constant_variables           = ['REAL_GO', 'REAL_CAP', 'REAL_LAB', 'REAL_II']\n",
    "qi_variables                 = ['GO_QI', 'CAP_QI', 'LAB_QI', 'II_QI']\n",
    "productivity_index_variables = ['TFP_GO', 'TFP_VA', 'LP_GO', 'LP_VA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf6f127",
   "metadata": {},
   "source": [
    "## 2. Merge the BEA-BLS data with US KLEMS 2017\n",
    "We combine data from US KLEMS (March 2017 Release) with BEA-BLS Integrated Industry-Level Production Account for 1947–2016, and BEA-BLS Integrated Industry-Level Production Account for 1997-2023.\n",
    "This produces two datsets containing GO, VA, CAP, LAB, II in nominal termns, real terms, as well as quantity indices, by industry for 44 industries between 1947 and 2023 and 63 industries for 1963-2016."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9d9eb",
   "metadata": {},
   "source": [
    "##### 2.1 Merge the two periods available for the *industry-production-account-experimental* dataset. \n",
    "This requires aggregating industries consistently first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d36af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = ['yr', 'indnum', 'go.', 'goqi.', 'ii.', 'iiqi.', 'vlcol.', 'vln.', 'vkit.', 'vksoft.', 'vkRD.', 'vkart.', 'vkoth.', 'qlindexcol_merge.', 'qlindexn_merge.', 'qkit.', 'qks.', 'qkrd.', 'qka.', 'qko.', 'hrs']\n",
    "\n",
    "data_1 = pd.read_excel(os.path.join(import_file_path,'industry-production-account-experimental.xlsx'), sheet_name='1947-1963', skiprows=1, usecols=required_columns)\n",
    "data_2 = pd.read_excel(os.path.join(import_file_path,'industry-production-account-experimental.xlsx'), sheet_name='1963-2016', skiprows=1, usecols=required_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c56089",
   "metadata": {},
   "source": [
    "Build quantity indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9018fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data_1.reset_index()\n",
    "data_2 = data_2.reset_index()\n",
    "\n",
    "q_vars_dict_1 = {\n",
    "    'GO':   ['goqi.'],\n",
    "    'CAP':  ['qkit.', 'qks.', 'qkrd.', 'qka.', 'qko.'],\n",
    "    'LAB':  ['qlindexcol_merge.', 'qlindexn_merge.'],\n",
    "    'II':   ['iiqi.']}\n",
    "\n",
    "v_vars_dict_1 = {\n",
    "    'GO':   ['go.'],\n",
    "    'CAP':  ['vkit.', 'vksoft.', 'vkRD.', 'vkart.', 'vkoth.'],\n",
    "    'LAB':  ['vlcol.', 'vln.'],\n",
    "    'II':   ['ii.']}\n",
    "\n",
    "for df in [data_1, data_2]:\n",
    "    for variable in core_variables:\n",
    "        q_vars = q_vars_dict_1[variable]\n",
    "        v_vars = v_vars_dict_1[variable]\n",
    "        df[f'{variable}_QI'] = tornqvist_index(df, q_vars, v_vars, industry_column='indnum')\n",
    "\n",
    "data_1['VA'] = data_1['go.'] - data_1['ii.']\n",
    "data_2['VA'] = data_2['go.'] - data_2['ii.']\n",
    "\n",
    "data_1.reset_index(inplace=True)\n",
    "data_2.reset_index(inplace=True)\n",
    "data_1 = data_1[qi_variables + ['hrs', 'VA', 'indnum', 'yr']]\n",
    "data_2 = data_2[qi_variables + ['hrs', 'VA', 'indnum', 'yr']]\n",
    "data_1 = data_1.rename(columns={'hrs': 'TOT_HRS', 'indnum': 'industry_id', 'yr': 'year'})\n",
    "data_2 = data_2.rename(columns={'hrs': 'TOT_HRS', 'indnum': 'industry_id', 'yr': 'year'})\n",
    "data_1 = data_1.set_index(['year', 'industry_id'])\n",
    "data_2 = data_2.set_index(['year', 'industry_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd90cf",
   "metadata": {},
   "source": [
    "Aggregate Industries following the BEA-BLS *industry-production-account-experimental* aggregation for 1947-1963."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30bd8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "qi_variables = ['GO_QI', 'CAP_QI', 'LAB_QI', 'II_QI', 'TOT_HRS']\n",
    "\n",
    "data_2 = aggregate_industries(data_2)\n",
    "data_2 = data_2.drop(columns=['VA'])\n",
    "data_1 = data_1.drop(columns=['VA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d8662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1.reset_index(inplace=True)\n",
    "data_2.reset_index(inplace=True)\n",
    "\n",
    "data_1['TOT_HRS']   = normalise(data_1, 'TOT_HRS', year=1947, industry_column='industry_id')\n",
    "data_2['TOT_HRS']   = normalise(data_2, 'TOT_HRS', year=1963, industry_column='industry_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1851030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qi_47_to_16    = chain(data_1, data_2, 1963)\n",
    "df_47_to_16_hours = df_qi_47_to_16['TOT_HRS'].copy()\n",
    "qi_variables      = ['GO_QI', 'CAP_QI', 'LAB_QI', 'II_QI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77097617",
   "metadata": {},
   "source": [
    "Chain together the two subperiods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03621f3d",
   "metadata": {},
   "source": [
    "##### 2.2 Merge the BEA data (1997 to 2023) with KLEMS2017 (1947 to 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15127835",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_qty_list  = ['Capital_Art_Quantity', 'Capital_R&D_Quantity', 'Capital_IT_Quantity', 'Capital_Other_Quantity', 'Capital_Software_Quantity']\n",
    "cap_comp_list = ['Capital_Art Compensation', 'Capital_R&D Compensation', 'Capital_IT Compensation', 'Capital_Other Compensation', 'Capital_Software Compensation']\n",
    "\n",
    "lab_qty_list  = ['Labor_Col_Quantity', 'Labor_NoCol_Quantity']\n",
    "lab_comp_list = ['Labor_Col Compensation', 'Labor_NoCol Compensation']\n",
    "\n",
    "ii_qty_list   = ['Energy_Quantity', 'Materials_Quantity', 'Services_Quantity']\n",
    "ii_comp_list  = ['Energy Compensation', 'Materials Compensation', 'Service Compensation']\n",
    "\n",
    "relevant_sheets = cap_qty_list + cap_comp_list + lab_qty_list + lab_comp_list + ii_qty_list + ii_comp_list + ['Value Added'] + ['VA_Quantity'] + ['Gross Output'] + ['Gross Output_Quantity'] + ['Labor Hours_Quantity']\n",
    "\n",
    "long_data = []\n",
    "\n",
    "for sheet in relevant_sheets:\n",
    "    df = pd.read_excel(os.path.join(import_file_path, 'industry-production-account-capital.xlsx'), sheet_name=sheet, header=[1])\n",
    "    df = df.dropna(how='all')\n",
    "    df.rename(columns={df.columns[0]: 'industry'}, inplace=True) \n",
    "    df_long = df.melt(id_vars='industry', var_name='Year', value_name=sheet)\n",
    "    long_data.append(df_long)\n",
    "\n",
    "df = reduce(lambda left, right: pd.merge(left, right, on=['industry', 'Year'], how='outer'), long_data)\n",
    "\n",
    "industry_order    = long_data[0]['industry'].drop_duplicates().tolist()\n",
    "industry_id_map   = {industry: i+1 for i, industry in enumerate(industry_order)}\n",
    "df['industry_id'] = df['industry'].map(industry_id_map)\n",
    "\n",
    "df = clean(df)\n",
    "cap_qty_list  = clean(cap_qty_list)\n",
    "cap_comp_list = clean(cap_comp_list)\n",
    "lab_qty_list  = clean(lab_qty_list)\n",
    "lab_comp_list = clean(lab_comp_list)\n",
    "ii_qty_list   = clean(ii_qty_list)\n",
    "ii_comp_list  = clean(ii_comp_list)\n",
    "\n",
    "df['year'] = df['year'].astype(int)\n",
    "df = df.set_index(['industry_id', 'year']).sort_index(level=['industry_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08e88357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_klems = pd.read_excel(os.path.join(import_file_path, 'usa_wk_mar_2017.xlsx'), sheet_name='KLEMdata', header=[1])\n",
    "df_klems = df_klems[['year', 'industry', 'gross output', 'capital', 'labor', 'intermediate']]\n",
    "df_klems = df_klems.rename(columns={'gross output': 'GO', 'capital': 'CAP', 'labor': 'LAB', 'intermediate': 'II', 'industry': 'industry_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06274d6f",
   "metadata": {},
   "source": [
    "Import and process KLEMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad47feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'gross_output': 'GO', 'value_added': 'VA', 'labor_hours_quantity': 'TOT_HRS'})\n",
    "df['LAB'] = df[lab_comp_list].sum(axis=1)\n",
    "df['CAP'] = df[cap_comp_list].sum(axis=1)\n",
    "df['II']  = df[ii_comp_list].sum(axis=1)\n",
    "\n",
    "df_post_2014 = df.copy()\n",
    "df_post_2014 = df_post_2014.reset_index()\n",
    "df_post_2014 = df_post_2014[core_variables + ['industry_id', 'year', 'TOT_HRS']]\n",
    "df_post_2014 = df_post_2014[df_post_2014['year'] > 2014]\n",
    "\n",
    "df_klems_62_63 = df_klems['industry_id'].isin([62, 63])\n",
    "df_62 = df_klems[df_klems_62_63].groupby('year').sum()\n",
    "df_62['industry_id'] = 62\n",
    "df_62 = df_62.reset_index()\n",
    "\n",
    "df_klems_64_65 = df_klems['industry_id'].isin([64, 65])\n",
    "df_63 = df_klems[df_klems_64_65].groupby('year').sum()\n",
    "df_63['industry_id'] = 63\n",
    "df_63 = df_63.reset_index()\n",
    "\n",
    "df_klems = df_klems[~df_klems['industry_id'].isin([62, 63, 64, 65])]\n",
    "df_klems = pd.concat([df_klems, df_62, df_63], ignore_index=True)\n",
    "\n",
    "df_nominal_47_to_23 = pd.concat([df_post_2014, df_klems], ignore_index=True)\n",
    "\n",
    "# nominal values in aggregate industries\n",
    "\n",
    "aggregate_dict = {}\n",
    "\n",
    "for agg_code, industries in aggregate_groups.items():\n",
    "    data_slice = df_nominal_47_to_23[df_nominal_47_to_23['industry_id'].isin(industries)].copy()\n",
    "    data_slice[core_variables] = data_slice.groupby('year')[core_variables].transform('sum')\n",
    "    data_slice = data_slice.drop_duplicates(subset='year')\n",
    "    data_slice['industry_id'] = agg_code\n",
    "    aggregate_dict[agg_code] = data_slice[['year', 'industry_id'] + core_variables]\n",
    "\n",
    "df_nominal_47_to_23 = pd.concat([df_nominal_47_to_23] + list(aggregate_dict.values()), ignore_index=True).set_index(['year', 'industry_id']).sort_index(level=['industry_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4ad6c",
   "metadata": {},
   "source": [
    "Nominal values first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b0341cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "\n",
    "q_vars_dict_2 = {\n",
    "    'GO': ['gross_output_quantity'],\n",
    "    'CAP': cap_qty_list,\n",
    "    'LAB': lab_qty_list,\n",
    "    'II':  ii_qty_list}\n",
    "\n",
    "v_vars_dict_2 = {\n",
    "    'GO':  ['GO'],\n",
    "    'CAP': cap_comp_list,\n",
    "    'LAB': lab_comp_list,\n",
    "    'II':  ii_comp_list}\n",
    "\n",
    "for variable in core_variables:\n",
    "    q_vars = q_vars_dict_2[variable]\n",
    "    v_vars = v_vars_dict_2[variable]\n",
    "    df[f'{variable}_QI'] = tornqvist_index(df, q_vars, v_vars, industry_column='industry_id')\n",
    "\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "df_qi_97_to_23 = df[qi_variables + ['year', 'industry_id', 'VA', 'TOT_HRS']].copy()\n",
    "df_qi_97_to_23['TOT_HRS'] = normalise(df, 'TOT_HRS', year=1997, industry_column='industry_id')\n",
    "df_qi_97_to_23 = df_qi_97_to_23.set_index(['year', 'industry_id']).sort_index(level=['industry_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc45e1",
   "metadata": {},
   "source": [
    "Quantity indices second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b0341cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qi_variables      = ['GO_QI', 'CAP_QI', 'LAB_QI', 'II_QI', 'TOT_HRS']\n",
    "df_qi_97_to_23    = aggregate_industries(df_qi_97_to_23)\n",
    "qi_variables      = ['GO_QI', 'CAP_QI', 'LAB_QI', 'II_QI']\n",
    "df_qi_97_to_23    = df_qi_97_to_23.drop(columns=['VA'])\n",
    "df_97_to_23_hours = df_qi_97_to_23['TOT_HRS'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0511be",
   "metadata": {},
   "source": [
    "#### 2.3 Chaining the data for 1947-2016 with 1997-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "252d29d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_47_to_96_hours = df_47_to_16_hours[df_47_to_16_hours.index.get_level_values('year').astype(int) <= 1997]\n",
    "qi_variables      = ['TOT_HRS']\n",
    "df_47_to_23_hours = chain(df_47_to_96_hours, df_97_to_23_hours, 1997)\n",
    "qi_variables      = ['GO_QI', 'CAP_QI', 'LAB_QI', 'II_QI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8305e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qi_47_to_96 = df_qi_47_to_16[df_qi_47_to_16.index.get_level_values('year').astype(int) <= 1997]\n",
    "df_qi_47_to_23 = chain(df_qi_47_to_96, df_qi_97_to_23, 1997)\n",
    "df_qi_47_to_23 = df_qi_47_to_23.reset_index()\n",
    "\n",
    "df_qi_47_to_23['year'] = df_qi_47_to_23['year'].astype(int)\n",
    "\n",
    "for qi in qi_variables:\n",
    "   df_qi_47_to_23[qi] = normalise(df_qi_47_to_23, qi, year=2009, industry_column='industry_id')\n",
    "\n",
    "df_qi_47_to_23 = df_qi_47_to_23.set_index(['year', 'industry_id']).sort_index(level=['industry_id'])\n",
    "df_47_to_23    = pd.merge(df_nominal_47_to_23[core_variables], df_qi_47_to_23[qi_variables],  left_index=True, right_index=True, how='inner').round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d106c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_47_to_23    = pd.merge(df_47_to_23, df_47_to_23_hours, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d43293",
   "metadata": {},
   "source": [
    "#### 2.4 Compute Variables in Real Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18643efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_47_to_23 = df_47_to_23.reset_index()\n",
    "\n",
    "for variable in core_variables:\n",
    "    df_47_to_23[f'REAL_{variable}'] = constant_values(df_47_to_23, variable, 2009)\n",
    "\n",
    "df_47_to_23 = df_47_to_23.set_index(['year', 'industry_id']).sort_index(level=['industry_id'])\n",
    "df_47_to_23 = df_47_to_23[core_variables + constant_variables + qi_variables + ['TOT_HRS']]\n",
    "df          = df_47_to_23.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071d354",
   "metadata": {},
   "source": [
    "#### 2.4 Compute Value Added"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dd88e",
   "metadata": {},
   "source": [
    "This follows the standard approach\n",
    "$$\n",
    "P_{V A}(t) Q_{V A}(t) = P_Y (t) Q_Y (t) - P_{II} (t) Q_{II} (t),\n",
    "$$\n",
    "wher $VA$ is Value Added, $Y$ is Gross Output, and $II$ is Itermediate Inputs. $P_{j}$ and $Q_{j}$ represent prices and quantities for component $j$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b145219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VA'] = df['GO'] - df['II']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112da32",
   "metadata": {},
   "source": [
    "## 3. Compute TFP and LP Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f0c82",
   "metadata": {},
   "source": [
    "Compute the share of VA in GO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc613dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['VA/GO'] = df['VA'] / df['GO']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0047b87",
   "metadata": {},
   "source": [
    "### 3.1 Build a VA Tornqvist Quantity Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe21eff",
   "metadata": {},
   "source": [
    "To compute a value added quantity index, we start from the definition of a Tornqvist Quantity Index for total output $Y$\n",
    "\n",
    "$$\n",
    "\\Delta \\ln Q_Y (t) = \\bar{\\nu}_{V A} (t) \\Delta \\ln Q_{VA}(t)   + \\bar{\\nu}_{II} (t) \\Delta \\ln Q_{II} (t),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Delta \\ln Q_{j}(t) = \\ln Q_{j}(t) - \\ln Q_{j}(t-1),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\bar{\\nu}_{j}(t) = 0.5 \\times \\left( \\frac{P_{j}(t) Q_{j}(t)}{P_Y (t) Q_Y (t)} + \\frac{P_{j}(t - 1) Q_{j}(t - 1)}{P_Y (t - 1) Q_Y (t - 1)} \\right)\n",
    "$$\n",
    "\n",
    "representing the Tornqvist weight for the component $j \\in \\left\\lbrace VA,II \\right\\rbrace$ of $GO$.\n",
    "\n",
    "Re arranging terms, we get:\n",
    "\n",
    "$$\n",
    "\\Delta \\ln Q_{VA}(t) = \\frac{\\Delta \\ln Q_Y (t) - \\bar{\\nu}_{II} (t) \\Delta \\ln Q_{II} (t)}{\\bar{\\nu}_{V A} (t)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b7582",
   "metadata": {},
   "source": [
    "Compute log differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a61a6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ln_REAL_GO'] = np.log(df['REAL_GO'])\n",
    "df['ln_REAL_II'] = np.log(df['REAL_II'])\n",
    "\n",
    "df['delta_ln_REAL_GO'] = delta(df, 'ln_REAL_GO')\n",
    "df['delta_ln_REAL_II'] = delta(df, 'ln_REAL_II')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96514a48",
   "metadata": {},
   "source": [
    "Compute the Tornqvist weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "110f81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['II/GO'] = df['II'] / df['GO']\n",
    "\n",
    "df['VA/GO_lag'] = lag(df, 'VA/GO')\n",
    "df['II/GO_lag'] = lag(df, 'II/GO')\n",
    "\n",
    "df['VA_tornqvist_GO_share'] = 0.5 * (df['VA/GO'] + df['VA/GO_lag'])\n",
    "df['II_tornqvist_GO_share'] = 0.5 * (df['II/GO'] + df['II/GO_lag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab08361",
   "metadata": {},
   "source": [
    "Compute the log change of the Value Added quantity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fefc0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta_ln_VA_QI'] = ((df['delta_ln_REAL_GO'] - (df['II_tornqvist_GO_share']*df['delta_ln_REAL_II']))/df['VA_tornqvist_GO_share'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86461156",
   "metadata": {},
   "source": [
    "### 3.2 Build Labor Productivity Indices (LP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be29c4ae",
   "metadata": {},
   "source": [
    "We follow KLEMS and use total hours as a measure of Labor Input. Therefore:\n",
    "\n",
    "* LP_VA:\n",
    "$$\n",
    "\\Delta \\ln LP_{VA}(t) = \\Delta \\ln Q_{VA}(t) - \\Delta \\ln Q_L(t)\n",
    "$$\n",
    "\n",
    "* LP_GO:\n",
    "$$\n",
    "\\Delta \\ln LP_{GO}(t) = \\Delta \\ln Q_{GO}(t) - \\Delta \\ln Q_L(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "777c3227",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ln_REAL_LAB']       = np.log(df['REAL_LAB'])\n",
    "df['delta_ln_REAL_LAB'] = delta(df, 'ln_REAL_LAB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b0920ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ln_hours']       = np.log(df['TOT_HRS'])\n",
    "df['delta_ln_hours'] = delta(df, 'ln_hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17660705",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta_ln_LP_VA'] = (df['delta_ln_VA_QI'] - df['delta_ln_hours'])\n",
    "df['delta_ln_LP_GO'] = (df['delta_ln_REAL_GO'] - df['delta_ln_hours'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fee178",
   "metadata": {},
   "source": [
    "### 3.3 Build Total Factor Productivity (TFP) Indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a89af5",
   "metadata": {},
   "source": [
    "Start with the Tornqvist Index for VA\n",
    "$$\n",
    "\\Delta \\ln Q_{VA}(t) = \\Delta \\ln TFP(t) + \\bar{\\psi}_L(t) \\Delta \\ln Q_L(t) + \\bar{\\psi}_K(t) \\Delta \\ln Q_K(t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803eee6",
   "metadata": {},
   "source": [
    "Re-arranging terms, we get\n",
    "$$\n",
    "\\Delta \\ln TFP(t) = \\Delta \\ln Q_{VA}(t) - \\bar{\\psi}_L(t) \\Delta \\ln Q_L(t) - \\bar{\\psi}_K(t) \\Delta \\ln Q_K(t),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\bar{\\psi}_{j}(t) = 0.5 \\times \\left( \\frac{P_{j}(t) Q_{j}(t)}{P_{VA}(t) Q_{VA}(t)} + \\frac{P_{j}(t-1) Q_{j}(t-1)}{P_{VA}(t-1) Q_{VA}(t-1)} \\right),\n",
    "$$\n",
    "\n",
    "for $j \\in \\left\\lbrace LAB,CAP \\right\\rbrace$ and  are nominal labor LAB ($L$) or nominal CAP ($K$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb057518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LAB/VA'] = df['LAB'] / df['VA']\n",
    "df['CAP/VA'] = df['CAP'] / df['VA']\n",
    "df['LAB/VA_lag'] = lag(df, 'LAB/VA')\n",
    "df['CAP/VA_lag'] = lag(df, 'CAP/VA')\n",
    "df['L_tornqvist_VA_share'] = 0.5 * (df['LAB/VA'] + df['LAB/VA_lag'])\n",
    "df['CAP_tornqvist_VA_share'] = 0.5 * (df['CAP/VA'] + df['CAP/VA_lag'])\n",
    "\n",
    "df['LAB/GO'] = df['LAB'] / df['GO']\n",
    "df['CAP/GO'] = df['CAP'] / df['GO']\n",
    "df['LAB/GO_lag'] = lag(df, 'LAB/GO')\n",
    "df['CAP/GO_lag'] = lag(df, 'CAP/GO')\n",
    "df['L_tornqvist_GO_share'] = 0.5 * (df['LAB/GO'] + df['LAB/GO_lag'])\n",
    "df['CAP_tornqvist_GO_share'] = 0.5 * (df['CAP/GO'] + df['CAP/GO_lag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "016a4a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ln_REAL_CAP'] = np.log(df['REAL_CAP'])\n",
    "df['delta_ln_REAL_CAP'] = delta(df, 'ln_REAL_CAP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36b959",
   "metadata": {},
   "source": [
    "Compute TFP growth rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a40d7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta_ln_TFP_VA'] = (df['delta_ln_VA_QI'] - (df['L_tornqvist_VA_share']*df['delta_ln_REAL_LAB']) - (df['CAP_tornqvist_VA_share']*df['delta_ln_REAL_CAP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ea5fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['delta_ln_TFP_GO'] = (df['delta_ln_REAL_GO'] - (df)['L_tornqvist_GO_share']*df['delta_ln_REAL_LAB'] - (df['CAP_tornqvist_GO_share']*df['delta_ln_REAL_CAP']) - (df['II_tornqvist_GO_share']*df['delta_ln_REAL_II']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aedaa3",
   "metadata": {},
   "source": [
    "Compute the TFP, LP and VA indexes. Normalise to 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62366c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_index(df, index_name):\n",
    "    df['ln_' + index_name] = df.groupby('industry_id')['delta_ln_' + index_name].cumsum().fillna(0)\n",
    "    df[index_name] = np.exp(df['ln_' + index_name])\n",
    "    df[index_name] *= 100\n",
    "    return df[index_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78fdb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "\n",
    "for variable in productivity_index_variables:\n",
    "    df[variable] = recover_index(df, variable)\n",
    "    df[variable] = normalise(df, variable, year=2009, industry_column='industry_id')\n",
    "df['VA_QI'] = recover_index(df, 'VA_QI')\n",
    "df['VA_QI'] = normalise(df, 'VA_QI', year=2009, industry_column='industry_id')\n",
    "\n",
    "df = df.set_index(['year', 'industry_id']).sort_index(level=['industry_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc66b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[core_variables + ['VA'] + constant_variables + qi_variables + ['VA_QI'] + productivity_index_variables].round(4)\n",
    "df_complete = df.copy()\n",
    "\n",
    "# Apply different rounding to different variable groups\n",
    "two_decimal_vars = ['GO', 'CAP', 'LAB', 'II', 'VA', 'REAL_GO', 'REAL_CAP', 'REAL_LAB', 'REAL_II']\n",
    "df_complete[two_decimal_vars] = df_complete[two_decimal_vars].round(2)\n",
    "\n",
    "# Round remaining variables to 4 decimal places\n",
    "four_decimal_vars = [col for col in df_complete.columns if col not in two_decimal_vars]\n",
    "df_complete[four_decimal_vars] = df_complete[four_decimal_vars].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80703810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "('year', 'industry_id')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "GO",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LAB",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "II",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "REAL_GO",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "REAL_CAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "REAL_LAB",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "REAL_II",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "GO_QI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "CAP_QI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LAB_QI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "II_QI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "VA_QI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TFP_GO",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TFP_VA",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LP_GO",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "LP_VA",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "717ca618-cdb9-4857-9a69-d054d8d9e14d",
       "rows": [
        [
         "(1947, 1)",
         "31299.23",
         "9744.23",
         "10039.0",
         "11516.0",
         "19783.23",
         "85420.74",
         "58072.43",
         "175614.44",
         "71655.42",
         "28.0331",
         "91.8022",
         "350.6423",
         "37.443",
         "18.6973",
         "34.2724",
         "9.9743",
         "4.7605",
         "3.1752"
        ],
        [
         "(1948, 1)",
         "35560.7",
         "11602.55",
         "11443.14",
         "12515.0",
         "23045.7",
         "93240.3",
         "62451.03",
         "169739.83",
         "71305.97",
         "30.5993",
         "98.724",
         "338.9127",
         "37.2604",
         "21.4984",
         "37.0196",
         "11.2514",
         "5.2976",
         "3.722"
        ],
        [
         "(1949, 1)",
         "29746.57",
         "6988.22",
         "11310.34",
         "11448.0",
         "18298.57",
         "90564.31",
         "62630.12",
         "170840.07",
         "70636.94",
         "29.7211",
         "99.0071",
         "341.1095",
         "36.9108",
         "20.6429",
         "35.9716",
         "10.7512",
         "5.2764",
         "3.6647"
        ],
        [
         "(1950, 1)",
         "31972.91",
         "9193.45",
         "10284.46",
         "12495.0",
         "19477.91",
         "95692.03",
         "63096.65",
         "158602.68",
         "74804.06",
         "31.4039",
         "99.7446",
         "316.6756",
         "39.0883",
         "21.7807",
         "38.0812",
         "11.7999",
         "5.8552",
         "4.061"
        ],
        [
         "(1951, 1)",
         "37255.44",
         "11954.3",
         "10551.13",
         "14750.0",
         "22505.44",
         "94865.65",
         "67334.63",
         "150626.21",
         "79642.33",
         "31.1327",
         "106.4441",
         "300.7493",
         "41.6165",
         "20.6164",
         "36.6797",
         "11.0925",
         "6.0988",
         "4.0387"
        ]
       ],
       "shape": {
        "columns": 18,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>GO</th>\n",
       "      <th>CAP</th>\n",
       "      <th>LAB</th>\n",
       "      <th>II</th>\n",
       "      <th>VA</th>\n",
       "      <th>REAL_GO</th>\n",
       "      <th>REAL_CAP</th>\n",
       "      <th>REAL_LAB</th>\n",
       "      <th>REAL_II</th>\n",
       "      <th>GO_QI</th>\n",
       "      <th>CAP_QI</th>\n",
       "      <th>LAB_QI</th>\n",
       "      <th>II_QI</th>\n",
       "      <th>VA_QI</th>\n",
       "      <th>TFP_GO</th>\n",
       "      <th>TFP_VA</th>\n",
       "      <th>LP_GO</th>\n",
       "      <th>LP_VA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>industry_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <th>1</th>\n",
       "      <td>31299.23</td>\n",
       "      <td>9744.23</td>\n",
       "      <td>10039.00</td>\n",
       "      <td>11516.0</td>\n",
       "      <td>19783.23</td>\n",
       "      <td>85420.74</td>\n",
       "      <td>58072.43</td>\n",
       "      <td>175614.44</td>\n",
       "      <td>71655.42</td>\n",
       "      <td>28.0331</td>\n",
       "      <td>91.8022</td>\n",
       "      <td>350.6423</td>\n",
       "      <td>37.4430</td>\n",
       "      <td>18.6973</td>\n",
       "      <td>34.2724</td>\n",
       "      <td>9.9743</td>\n",
       "      <td>4.7605</td>\n",
       "      <td>3.1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <th>1</th>\n",
       "      <td>35560.70</td>\n",
       "      <td>11602.55</td>\n",
       "      <td>11443.14</td>\n",
       "      <td>12515.0</td>\n",
       "      <td>23045.70</td>\n",
       "      <td>93240.30</td>\n",
       "      <td>62451.03</td>\n",
       "      <td>169739.83</td>\n",
       "      <td>71305.97</td>\n",
       "      <td>30.5993</td>\n",
       "      <td>98.7240</td>\n",
       "      <td>338.9127</td>\n",
       "      <td>37.2604</td>\n",
       "      <td>21.4984</td>\n",
       "      <td>37.0196</td>\n",
       "      <td>11.2514</td>\n",
       "      <td>5.2976</td>\n",
       "      <td>3.7220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <th>1</th>\n",
       "      <td>29746.57</td>\n",
       "      <td>6988.22</td>\n",
       "      <td>11310.34</td>\n",
       "      <td>11448.0</td>\n",
       "      <td>18298.57</td>\n",
       "      <td>90564.31</td>\n",
       "      <td>62630.12</td>\n",
       "      <td>170840.07</td>\n",
       "      <td>70636.94</td>\n",
       "      <td>29.7211</td>\n",
       "      <td>99.0071</td>\n",
       "      <td>341.1095</td>\n",
       "      <td>36.9108</td>\n",
       "      <td>20.6429</td>\n",
       "      <td>35.9716</td>\n",
       "      <td>10.7512</td>\n",
       "      <td>5.2764</td>\n",
       "      <td>3.6647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <th>1</th>\n",
       "      <td>31972.91</td>\n",
       "      <td>9193.45</td>\n",
       "      <td>10284.46</td>\n",
       "      <td>12495.0</td>\n",
       "      <td>19477.91</td>\n",
       "      <td>95692.03</td>\n",
       "      <td>63096.65</td>\n",
       "      <td>158602.68</td>\n",
       "      <td>74804.06</td>\n",
       "      <td>31.4039</td>\n",
       "      <td>99.7446</td>\n",
       "      <td>316.6756</td>\n",
       "      <td>39.0883</td>\n",
       "      <td>21.7807</td>\n",
       "      <td>38.0812</td>\n",
       "      <td>11.7999</td>\n",
       "      <td>5.8552</td>\n",
       "      <td>4.0610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <th>1</th>\n",
       "      <td>37255.44</td>\n",
       "      <td>11954.30</td>\n",
       "      <td>10551.13</td>\n",
       "      <td>14750.0</td>\n",
       "      <td>22505.44</td>\n",
       "      <td>94865.65</td>\n",
       "      <td>67334.63</td>\n",
       "      <td>150626.21</td>\n",
       "      <td>79642.33</td>\n",
       "      <td>31.1327</td>\n",
       "      <td>106.4441</td>\n",
       "      <td>300.7493</td>\n",
       "      <td>41.6165</td>\n",
       "      <td>20.6164</td>\n",
       "      <td>36.6797</td>\n",
       "      <td>11.0925</td>\n",
       "      <td>6.0988</td>\n",
       "      <td>4.0387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        GO       CAP       LAB       II        VA   REAL_GO  \\\n",
       "year industry_id                                                              \n",
       "1947 1            31299.23   9744.23  10039.00  11516.0  19783.23  85420.74   \n",
       "1948 1            35560.70  11602.55  11443.14  12515.0  23045.70  93240.30   \n",
       "1949 1            29746.57   6988.22  11310.34  11448.0  18298.57  90564.31   \n",
       "1950 1            31972.91   9193.45  10284.46  12495.0  19477.91  95692.03   \n",
       "1951 1            37255.44  11954.30  10551.13  14750.0  22505.44  94865.65   \n",
       "\n",
       "                  REAL_CAP   REAL_LAB   REAL_II    GO_QI    CAP_QI    LAB_QI  \\\n",
       "year industry_id                                                               \n",
       "1947 1            58072.43  175614.44  71655.42  28.0331   91.8022  350.6423   \n",
       "1948 1            62451.03  169739.83  71305.97  30.5993   98.7240  338.9127   \n",
       "1949 1            62630.12  170840.07  70636.94  29.7211   99.0071  341.1095   \n",
       "1950 1            63096.65  158602.68  74804.06  31.4039   99.7446  316.6756   \n",
       "1951 1            67334.63  150626.21  79642.33  31.1327  106.4441  300.7493   \n",
       "\n",
       "                    II_QI    VA_QI   TFP_GO   TFP_VA   LP_GO   LP_VA  \n",
       "year industry_id                                                      \n",
       "1947 1            37.4430  18.6973  34.2724   9.9743  4.7605  3.1752  \n",
       "1948 1            37.2604  21.4984  37.0196  11.2514  5.2976  3.7220  \n",
       "1949 1            36.9108  20.6429  35.9716  10.7512  5.2764  3.6647  \n",
       "1950 1            39.0883  21.7807  38.0812  11.7999  5.8552  4.0610  \n",
       "1951 1            41.6165  20.6164  36.6797  11.0925  6.0988  4.0387  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4248d9a",
   "metadata": {},
   "source": [
    "## 4. Export the Data to an Excel Spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24a837be",
   "metadata": {},
   "outputs": [],
   "source": [
    "industries_info = (\n",
    "    pd.DataFrame(list(industry_id_map.items()), columns=['industry_name', 'industry_id'])\n",
    "    .sort_values('industry_id')\n",
    "    .reset_index(drop=True))\n",
    "\n",
    "variables_dictionary = {\n",
    "    '*': 'Note: variables with * are normalised to 2009 = 100',\n",
    "    'GO': 'Nominal Gross Output',\n",
    "    'CAP': 'Nominal Capital',\n",
    "    'LAB': 'Nominal Labor',\n",
    "    'II': 'Nominal Intermediate Inputs',\n",
    "    'VA': 'Nominal Value Added',\n",
    "    'REAL_GO*': 'Real Gross Output',\n",
    "    'REAL_CAP*': 'Real Capital',\n",
    "    'REAL_LAB*': 'Real Labor',\n",
    "    'REAL_II*': 'Real Intermediate Inputs',\n",
    "    'GO_QI*': 'Gross Output Quantity Index',\n",
    "    'CAP_QI*': 'Capital Quantity Index',\n",
    "    'LAB_QI*': 'Labor Quantity Index',\n",
    "    'II_QI*': 'Intermediate Inputs Quantity Index',\n",
    "    'VA_QI*': 'Value Added Quantity Index',\n",
    "    'TFP_GO*': 'Total Factor Productivity Index (GO)',\n",
    "    'TFP_VA*': 'Total Factor Productivity Index (VA)',\n",
    "    'LP_GO*': 'Labor Productivity Index (GO)',\n",
    "    'LP_VA*': 'Labor Productivity Index (VA)',\n",
    "}\n",
    "\n",
    "variables_info = pd.DataFrame(\n",
    "    list(variables_dictionary.items()), \n",
    "    columns=['variable_name', 'variable_description'])\n",
    "\n",
    "aggregate_industries_dictionary = {\n",
    "    'Period': '1947-2023',\n",
    "    '2936': 'Industries 29-36',\n",
    "    '3740': 'Industries 37-40',\n",
    "    '4144': 'Industries 41-44',\n",
    "    '4749': 'Industries 47-49',\n",
    "    '5152': 'Industries 51-52',\n",
    "    '5456': 'Industries 54-56',\n",
    "    '5758': 'Industries 57-58'\n",
    "}\n",
    "\n",
    "aggregate_industries_info = pd.DataFrame(\n",
    "    list(aggregate_industries_dictionary.items()),\n",
    "    columns=['industry_group', 'industry_ids']\n",
    ")\n",
    "\n",
    "# Create dataset for 1963-2023 with individual industries (1-63)\n",
    "aggregated_industry_ids = [2936, 3740, 4144, 4749, 5152, 5456, 5758]\n",
    "df_individual_industries = df_complete.copy()\n",
    "\n",
    "# Filter out aggregated industries and keep individual industries 1-63\n",
    "df_individual_industries = df_individual_industries.reset_index()\n",
    "df_individual_industries = df_individual_industries[\n",
    "    (~df_individual_industries['industry_id'].isin(aggregated_industry_ids)) & \n",
    "    (df_individual_industries['year'] >= 1963)\n",
    "]\n",
    "\n",
    "# Apply the same decimal formatting to the individual industries dataset\n",
    "for col in two_decimal_vars:\n",
    "    if col in df_individual_industries.columns:\n",
    "        df_individual_industries[col] = df_individual_industries[col].apply(lambda x: float(f\"{x:.2f}\"))\n",
    "\n",
    "for col in four_decimal_vars:\n",
    "    if col in df_individual_industries.columns:\n",
    "        df_individual_industries[col] = df_individual_industries[col].apply(lambda x: float(f\"{x:.4f}\"))\n",
    "\n",
    "df_individual_industries = df_individual_industries.set_index(['year', 'industry_id']).sort_index(level=['industry_id'])\n",
    "\n",
    "# Create dataset for 1947-2023 with properly organized aggregated industries\n",
    "# Define which individual industries should be removed (they're part of aggregates)\n",
    "individual_industries_to_remove = []\n",
    "for industries_list in aggregate_groups.values():\n",
    "    individual_industries_to_remove.extend(industries_list)\n",
    "\n",
    "df_aggregated = df_complete.copy().reset_index()\n",
    "\n",
    "# Remove individual industries that are part of aggregates\n",
    "df_aggregated = df_aggregated[~df_aggregated['industry_id'].isin(individual_industries_to_remove)]\n",
    "\n",
    "# Create a custom sorting order for industries\n",
    "def get_sort_key(industry_id):\n",
    "    \"\"\"Create sort key to place aggregates in correct positions\"\"\"\n",
    "    if industry_id <= 28:\n",
    "        return industry_id\n",
    "    elif industry_id == 2936:  # Should come after 28\n",
    "        return 28.5\n",
    "    elif industry_id <= 36:\n",
    "        return industry_id\n",
    "    elif industry_id == 3740:  # Should come after 36 (but we removed individual 29-36)\n",
    "        return 36.5\n",
    "    elif industry_id <= 40:\n",
    "        return industry_id\n",
    "    elif industry_id == 4144:  # Should come after 40\n",
    "        return 40.5\n",
    "    elif industry_id <= 46:\n",
    "        return industry_id\n",
    "    elif industry_id == 4749:  # Should come after 46 (we removed 47-49)\n",
    "        return 46.5\n",
    "    elif industry_id <= 50:\n",
    "        return industry_id\n",
    "    elif industry_id == 5152:  # Should come after 50\n",
    "        return 50.5\n",
    "    elif industry_id <= 53:\n",
    "        return industry_id\n",
    "    elif industry_id == 5456:  # Should come after 53\n",
    "        return 53.5\n",
    "    elif industry_id == 5758:  # Should come after 5456\n",
    "        return 53.6\n",
    "    else:\n",
    "        return industry_id\n",
    "\n",
    "# Apply custom sorting - by industry first, then by year\n",
    "df_aggregated['sort_key'] = df_aggregated['industry_id'].apply(get_sort_key)\n",
    "df_aggregated = df_aggregated.sort_values(['sort_key', 'year']).drop('sort_key', axis=1)\n",
    "\n",
    "# Apply the same decimal formatting to the aggregated dataset\n",
    "for col in two_decimal_vars:\n",
    "    if col in df_aggregated.columns:\n",
    "        df_aggregated[col] = df_aggregated[col].apply(lambda x: float(f\"{x:.2f}\"))\n",
    "\n",
    "for col in four_decimal_vars:\n",
    "    if col in df_aggregated.columns:\n",
    "        df_aggregated[col] = df_aggregated[col].apply(lambda x: float(f\"{x:.4f}\"))\n",
    "\n",
    "df_aggregated = df_aggregated.set_index(['year', 'industry_id'])\n",
    "\n",
    "# Export both datasets to the same Excel file with separate sheets\n",
    "file_path = os.path.join(export_file_path, \"EV_production_accounts_1947to2023.xlsx\")\n",
    "\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "    # Info sheet\n",
    "    industries_info.to_excel(writer, sheet_name='Info', index=False, startrow=1, startcol=1)\n",
    "    aggregate_industries_info.to_excel(writer, sheet_name='Info', index=False, startrow=1, startcol=4)\n",
    "    variables_info.to_excel(writer, sheet_name='Info', index=False, startrow=1, startcol=7)\n",
    "    \n",
    "    # Data sheets\n",
    "    df_aggregated.to_excel(writer, sheet_name='Data_44Ind_1947-2023', index=True)\n",
    "    df_individual_industries.to_excel(writer, sheet_name='Data_63Ind_1963-2023', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
